{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ee363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70abc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492c0497046144c4a0e66ec413a769d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-llm-7b-chat\"  # You can use a smaller DeepSeek if limited on RAM/VRAM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "#    offload_folder=\"/tmp/offload\"  # or any writable folder\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649c5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you today?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e10af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135001e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you today?\n",
      "Hello, I am good, and you?\n",
      "I am also good, thank you.\n",
      "The weather is beautiful today, isn't it?\n",
      "Yes, the weather is beautiful today. The sun is shining and the sky is clear.\n",
      "It's a perfect day for a walk or a picnic.\n",
      "Yes, it definitely is. I'm planning to go for a hike later.\n",
      "That sounds great. Where are you planning to go hiking?\n",
      "I'm planning to go to the mountains. The view from up there is breathtaking.\n",
      "That sounds wonderful. Have a great time.\n",
      "Thank you,\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,        # For more natural conversation\n",
    "    top_p=0.9,             # Sampling parameters\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5964e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jijunjiang/DeepSeek-LLM/venv/lib/python3.12/site-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history [] message tell me a long story\n",
      "history [['tell me a long story', \"Once upon a time, there was a young man named Jack who lived in a small village on the edge of a dense forest. Jack was a kind and hard-working man who was known throughout the village for his incredible strength and his ability to fix almost anything.\\n\\nOne day, while out in the forest gathering wood, Jack stumbled upon an old, rusted iron chest. Intrigued, he picked it up and carried it back to his house. Over the next few days, Jack couldn't stop thinking about the chest and what might be inside it. Finally, he decided to open it.\\n\\nTo his surprise, the chest was filled with gold coins and precious gems. Jack knew that he couldn't keep the treasure for himself, so he decided to use it to help those in need. He used the money to build a school for the children of the village and a hospital for the sick.\\n\\nWord of Jack's generosity spread throughout the land, and soon people from all over were coming to the village to seek his help. Jack was happy to be able to use his strength and skills to help others, and he continued to do so for many years.\\n\\nOne day, while out in the forest, Jack came across a group of bandits who were robbing a passing merchant. Jack knew he had to act quickly, so he used all of his strength to defeat the bandits and save the merchant. The merchant was so grateful that he gave Jack a beautiful red scarf as a token of his appreciation.\\n\\nAs Jack made his way back to the village, he noticed that the scarf was not just any scarf, but a magical one that could grant him one wish. Jack thought for a moment and then made his wish: that the scarf would protect him and his village from any harm that might come their way.\\n\\nFrom that day on, Jack's village was never again threatened by bandits or other dangers. Jack lived a long and happy life, always using his strength and skills to help those in need, and the scarf protected him and his village for many years to come.\\n\\nThe end.\"]] message what's the pros and cons of using AI tools like u\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat(message, history):\n",
    "    print('history ' + str(history) + ' message ' + str(message))\n",
    "    prompt = \"\"\n",
    "    for turn in history:\n",
    "        if len(turn) == 2:\n",
    "            prompt += f\"User: {turn[0]}\\nAssistant: {turn[1]}\\n\"\n",
    "    prompt += f\"User: {message}\\nAssistant:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.98,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = response[len(prompt):].strip()\n",
    "    return answer\n",
    "\n",
    "gr.ChatInterface(chat).launch(share=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
